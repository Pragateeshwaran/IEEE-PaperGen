{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "def convert(path):\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "    model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True, cache_dir=\"D:\\hugging-models\"\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        max_new_tokens=128,\n",
    "        chunk_length_s=30,\n",
    "        batch_size=16,\n",
    "        return_timestamps=True,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "\n",
    "    result = pipe(path)\n",
    "    print(result)\n",
    "    # os.remove(path)\n",
    "    return result \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:697: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \" This is an AI agent doing financial analysis. I used it to build an LLM-Langchain app that could analyze financial statements. In fact, it's doing it without fine-tuning on this 300-page banking annual report right now. I'm going to teach you how to build it in 5 minutes. I went down the rabbit hole to discover how to use your own documents to build AI-powered large language model apps with Langchain. And the results speak for themselves. But first of all, why should we even bother trying to use our own documents? We've already built our own large language model app with Streamlit. Why the need for docos? Well, it gives you the ultimate flexibility to leverage large language models for tasks that are specific to you. Provide meeting minutes and create a summary. You could upload an assignment outline and have a response generated. Upload an annual report and have a large language model do the financial analysis for you. In fact, that's what we're going to be doing we're going to be building our very own personal ai powered investment banker using streamlit and our own annual report to get started we first need to install some dependencies namely langchain openai streamlit tick token chroma db pi pdf and crypto dome while we're at it we're going to need to get an openai api key it's optional to use the openai service you could substitute substitute it out for a Lama CPP or one of the Hugging Face hosted models. If you want a vid on that, let me know. Time to create our app. First up, in a new Python file called app.py, we're going to import some initial dependencies. OS is going to be used to set the API key. From langchain.llms, we'll import the OpenAI service. This will be the core LLM we use. We'll also bring in Streamlit while we're at it. Once that's done, we can set the API key and create the OpenAI service as an LLM. If we wanted to, we could tune the temperature depending on how creative or objective we wanted our responses. We also need somewhere to pass the prompt. We can use the Streamlit.TextInput element to do this. If our user hits enter, we need a way to send the prompt to the LLM. Let's throw in an if statement here and write out the text to the screen using streamlit.write. To start the app it's relatively straightforward, just write streamlit run app.py or whatever your python file is named. Side note, all this code is going to be available in the description below so you can give it a crack yourself. This gives us a baseline app and uses open AI to generate responses. But there's one key issue here, we haven't gone and used our own documents yet. Let's fix that but before we do, a word from our sponsors. Me. If you'd like to get up and running with Python for machine learning, head over to go.coursesfromnick.com forward slash Python, where you can take my end-to-end tech fundamentals course. Or if you want to dive straight into the deep end, you can check out my full stack machine learning course at this link here. And use YouTube 50 to get 50% off right now. Back to the video. We need At the top of our script, we're going to bring in PyPDFLoader from langchain.documentloaders and Chroma from langchain.vectorstores. PyPDFLoader is used to load and pass a PDF into memory. Chroma is a vector store that is critical to using your own documents. What actually happens, as far as I know, is your document is tokenized and loaded into Chroma for querying later on. This allows you to perform similarity search using similarity metrics like Euclidean distance, similarity metrics like Euclidean distance. Also similar to how Pinecone works. We'll do this in a sec. All right, time to load up our document. Using the PyPDF loader class, we're going to upload a banking annual report. I've got the files stored in the same place as my Python script, so I can just pass the name of the document direct to the class. If you wanted to use your own document, you could sub in the name of the file here. In this case, it's got to be a PDF, be a PDF but there are other loaders available inside of Langchain. We can then load the document into Chroma using the from documents method. To do this pass the pages from the PDF to the loader method. And then back to that similarity search. Using a Streamlit expander class we can search through the document in natural text and render the results to the screen. Say we ask about the performance of the bank. Chroma will return the relevant passages from the document loaded. These will eventually be passed through to the Langchain agent to generate a human-like response based on that context. But we don't need to do this manually. We can use the VectorStore agent from Langchain. This will package it all up pretty nicely. Last set of imports, home stretch now. First up, we'll import create VectorStore agent, VectorStore toolkit, and VectorStore info from Langchain.agents.agent toolkits. The vector store info class as far as I can tell just provides context about the store aka the pdf that we're going to pass through to our llm agent. You just pass your name, description and chroma store to that specific class. Then the magic happens. We can pass the vector store info wrapper to vector store toolkit. This makes the pdf available as a tool to langchain similar to how we use the wikipedia toolkit in the langchain crash course video. And last but not least bring it it together with the VectorStore agent. This is the most critical part. So the agent creator class will pass through our original OpenAI LLM service and the VectorStore toolkit. This packages it all up nicely and will in effect give our large language model agent access to our PDF. Rather than just outputting responses from the LLM like we had previously, we can now use the agent and run the prompt through it and again write out the responses to the screen this allows us to ask things like what was the net profit of the company with the response that we're getting back from our gpt investment banker being 4 706 million or 56 on the prior year which just so happens to match note 6 on earnings per share in the notes to the financial statements. What initiatives did the bank take towards sustainability? Our model is calling out the Net Zero Asset Managers Initiative and the Net Zero Banking Alliance. This is pretty closely tied to the governance section in the financial statement as well. And last but not least, summarize the financial performance of the bank. The model calls out net profit after taxes increasing by 56% compared to the prior year and EPS increasing by 51%. This just so happens to match the letter from the chair of the board of the remuneration committee calling out those exact same NPAT and EPS numbers. The document uses roughly 300 pages in length so documents can take a little while to return a response. That being said in 45 lines of code we've got our very own personal investment banker. Not too shabby. If you want to check out the lane crash course video I did go and click here.\", 'chunks': [{'timestamp': (0.08, 2.36), 'text': ' This is an AI agent doing financial analysis.'}, {'timestamp': (2.36, 5.84), 'text': ' I used it to build an LLM-Langchain app that could analyze financial statements.'}, {'timestamp': (5.84, 9.92), 'text': \" In fact, it's doing it without fine-tuning on this 300-page banking annual report right\"}, {'timestamp': (9.92, 10.92), 'text': ' now.'}, {'timestamp': (10.92, 12.04), 'text': \" I'm going to teach you how to build it in 5 minutes.\"}, {'timestamp': (12.04, 15.84), 'text': ' I went down the rabbit hole to discover how to use your own documents to build AI-powered'}, {'timestamp': (15.84, 18.4), 'text': ' large language model apps with Langchain.'}, {'timestamp': (18.4, 25.8), 'text': \" And the results speak for themselves. But first of all, why should we even bother trying to use our own documents? We've already built our own large language model app with Streamlit.\"}, {'timestamp': (25.8, 26.8), 'text': ' Why the need for docos?'}, {'timestamp': (26.8, 30.74), 'text': ' Well, it gives you the ultimate flexibility to leverage large language models for tasks'}, {'timestamp': (30.74, 32.68), 'text': ' that are specific to you.'}, {'timestamp': (32.68, 34.26), 'text': ' Provide meeting minutes and create a summary.'}, {'timestamp': (34.26, 37.18), 'text': ' You could upload an assignment outline and have a response generated.'}, {'timestamp': (37.18, 50.24), 'text': \" Upload an annual report and have a large language model do the financial analysis for you. In fact, that's what we're going to be doing we're going to be building our very own personal ai powered investment banker using streamlit and our own annual report to get\"}, {'timestamp': (50.24, 54.88), 'text': ' started we first need to install some dependencies namely langchain openai streamlit tick token'}, {'timestamp': (54.88, 60.64), 'text': \" chroma db pi pdf and crypto dome while we're at it we're going to need to get an openai api key it's\"}, {'timestamp': (60.64, 65.22), 'text': ' optional to use the openai service you could substitute substitute it out for a Lama CPP or one'}, {'timestamp': (65.22, 68.46), 'text': ' of the Hugging Face hosted models. If you want a vid on that, let me know. Time to create our app.'}, {'timestamp': (68.5, 72.36), 'text': \" First up, in a new Python file called app.py, we're going to import some initial dependencies.\"}, {'timestamp': (72.6, 78.18), 'text': \" OS is going to be used to set the API key. From langchain.llms, we'll import the OpenAI service.\"}, {'timestamp': (86.12, 89.96), 'text': \" This will be the core LLM we use. We'll also bring in Streamlit while we're at it. Once that's done, we can set the API key and create the OpenAI service as an LLM. If we wanted to, we could tune the temperature depending on how creative or objective we\"}, {'timestamp': (89.96, 90.96), 'text': ' wanted our responses.'}, {'timestamp': (90.96, 93.1), 'text': ' We also need somewhere to pass the prompt.'}, {'timestamp': (93.1, 95.68), 'text': ' We can use the Streamlit.TextInput element to do this.'}, {'timestamp': (95.68, 106.94), 'text': \" If our user hits enter, we need a way to send the prompt to the LLM. Let's throw in an if statement here and write out the text to the screen using streamlit.write. To start the app it's relatively straightforward, just write streamlit run app.py or whatever\"}, {'timestamp': (106.94, 108.32), 'text': ' your python file is named.'}, {'timestamp': (108.32, 111.36), 'text': ' Side note, all this code is going to be available in the description below so you can give it'}, {'timestamp': (111.36, 112.36), 'text': ' a crack yourself.'}, {'timestamp': (112.36, 115.84), 'text': ' This gives us a baseline app and uses open AI to generate responses.'}, {'timestamp': (115.84, 119.76), 'text': \" But there's one key issue here, we haven't gone and used our own documents yet.\"}, {'timestamp': (119.76, 127.5), 'text': \" Let's fix that but before we do, a word from our sponsors. Me. If you'd like to get up and running with Python for machine learning, head over to go.coursesfromnick.com forward slash Python,\"}, {'timestamp': (127.5, 130.0), 'text': ' where you can take my end-to-end tech fundamentals course.'}, {'timestamp': (130.0, 132.18), 'text': ' Or if you want to dive straight into the deep end,'}, {'timestamp': (132.18, 134.38), 'text': ' you can check out my full stack machine learning course'}, {'timestamp': (134.38, 135.34), 'text': ' at this link here.'}, {'timestamp': (135.34, 137.8), 'text': ' And use YouTube 50 to get 50% off right now.'}, {'timestamp': (137.8, 146.8), 'text': \" Back to the video. We need At the top of our script, we're going to bring in PyPDFLoader from langchain.documentloaders and Chroma from langchain.vectorstores.\"}, {'timestamp': (147.22, 149.58), 'text': ' PyPDFLoader is used to load and pass a PDF into memory.'}, {'timestamp': (149.84, 153.14), 'text': ' Chroma is a vector store that is critical to using your own documents.'}, {'timestamp': (153.24, 156.68), 'text': ' What actually happens, as far as I know, is your document is tokenized and loaded into'}, {'timestamp': (156.68, 157.98), 'text': ' Chroma for querying later on.'}, {'timestamp': (158.04, 162.08), 'text': ' This allows you to perform similarity search using similarity metrics like Euclidean distance,'}, {'timestamp': (165.06, 168.44), 'text': \" similarity metrics like Euclidean distance. Also similar to how Pinecone works. We'll do this in a sec. All right, time to load up our document. Using the PyPDF loader class, we're going to\"}, {'timestamp': (168.44, 172.42), 'text': \" upload a banking annual report. I've got the files stored in the same place as my Python script,\"}, {'timestamp': (172.52, 176.72), 'text': ' so I can just pass the name of the document direct to the class. If you wanted to use your'}, {'timestamp': (176.72, 180.58), 'text': \" own document, you could sub in the name of the file here. In this case, it's got to be a PDF,\"}, {'timestamp': (184.0, 189.2), 'text': ' be a PDF but there are other loaders available inside of Langchain. We can then load the document into Chroma using the from documents method. To do this pass the pages from the PDF to the'}, {'timestamp': (189.2, 193.2), 'text': ' loader method. And then back to that similarity search. Using a Streamlit expander class we can'}, {'timestamp': (193.2, 197.44), 'text': ' search through the document in natural text and render the results to the screen. Say we ask about'}, {'timestamp': (197.44, 206.88), 'text': ' the performance of the bank. Chroma will return the relevant passages from the document loaded. These will eventually be passed through to the Langchain agent to generate a human-like response'}, {'timestamp': (206.88, 208.18), 'text': ' based on that context.'}, {'timestamp': (208.18, 209.56), 'text': \" But we don't need to do this manually.\"}, {'timestamp': (209.56, 211.9), 'text': ' We can use the VectorStore agent from Langchain.'}, {'timestamp': (211.9, 213.56), 'text': ' This will package it all up pretty nicely.'}, {'timestamp': (213.56, 215.12), 'text': ' Last set of imports, home stretch now.'}, {'timestamp': (215.12, 217.28), 'text': \" First up, we'll import create VectorStore agent,\"}, {'timestamp': (217.28, 225.48), 'text': ' VectorStore toolkit, and VectorStore info from Langchain.agents.agent toolkits. The vector store info class as far as I can tell just provides context about the'}, {'timestamp': (225.48, 229.64), 'text': \" store aka the pdf that we're going to pass through to our llm agent. You just pass your name,\"}, {'timestamp': (229.74, 234.82), 'text': ' description and chroma store to that specific class. Then the magic happens. We can pass the'}, {'timestamp': (234.82, 239.46), 'text': ' vector store info wrapper to vector store toolkit. This makes the pdf available as a tool to'}, {'timestamp': (239.46, 243.6), 'text': ' langchain similar to how we use the wikipedia toolkit in the langchain crash course video.'}, {'timestamp': (246.0, 246.16), 'text': ' And last but not least bring it it together with the VectorStore agent.'}, {'timestamp': (247.46, 247.88), 'text': ' This is the most critical part.'}, {'timestamp': (251.7, 253.32), 'text': ' So the agent creator class will pass through our original OpenAI LLM service and the VectorStore toolkit.'}, {'timestamp': (253.68, 256.36), 'text': ' This packages it all up nicely and will in effect'}, {'timestamp': (256.36, 259.26), 'text': ' give our large language model agent access to our PDF.'}, {'timestamp': (259.54, 262.64), 'text': ' Rather than just outputting responses from the LLM like we had previously,'}, {'timestamp': (269.36, 273.12), 'text': \" we can now use the agent and run the prompt through it and again write out the responses to the screen this allows us to ask things like what was the net profit of the company with the response that we're getting back from our gpt\"}, {'timestamp': (273.12, 280.16), 'text': ' investment banker being 4 706 million or 56 on the prior year which just so happens to match note 6'}, {'timestamp': (280.16, 286.22), 'text': ' on earnings per share in the notes to the financial statements. What initiatives did the bank take towards sustainability?'}, {'timestamp': (286.66, 291.28), 'text': ' Our model is calling out the Net Zero Asset Managers Initiative and the Net Zero Banking'}, {'timestamp': (291.28, 294.72), 'text': ' Alliance. This is pretty closely tied to the governance section in the financial statement'}, {'timestamp': (294.72, 298.0), 'text': ' as well. And last but not least, summarize the financial performance of the bank.'}, {'timestamp': (298.64, 303.62), 'text': ' The model calls out net profit after taxes increasing by 56% compared to the prior year'}, {'timestamp': (303.62, 305.7), 'text': ' and EPS increasing by 51%.'}, {'timestamp': (305.7, 309.4), 'text': ' This just so happens to match the letter from the chair of the board of the remuneration committee'}, {'timestamp': (309.4, 315.82), 'text': ' calling out those exact same NPAT and EPS numbers. The document uses roughly 300 pages in length so'}, {'timestamp': (315.82, 319.96), 'text': ' documents can take a little while to return a response. That being said in 45 lines of code'}, {'timestamp': (319.96, 323.74), 'text': \" we've got our very own personal investment banker. Not too shabby. If you want to check out the lane\"}, {'timestamp': (323.74, 325.82), 'text': ' crash course video I did go and click here.'}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': \" This is an AI agent doing financial analysis. I used it to build an LLM-Langchain app that could analyze financial statements. In fact, it's doing it without fine-tuning on this 300-page banking annual report right now. I'm going to teach you how to build it in 5 minutes. I went down the rabbit hole to discover how to use your own documents to build AI-powered large language model apps with Langchain. And the results speak for themselves. But first of all, why should we even bother trying to use our own documents? We've already built our own large language model app with Streamlit. Why the need for docos? Well, it gives you the ultimate flexibility to leverage large language models for tasks that are specific to you. Provide meeting minutes and create a summary. You could upload an assignment outline and have a response generated. Upload an annual report and have a large language model do the financial analysis for you. In fact, that's what we're going to be doing we're going to be building our very own personal ai powered investment banker using streamlit and our own annual report to get started we first need to install some dependencies namely langchain openai streamlit tick token chroma db pi pdf and crypto dome while we're at it we're going to need to get an openai api key it's optional to use the openai service you could substitute substitute it out for a Lama CPP or one of the Hugging Face hosted models. If you want a vid on that, let me know. Time to create our app. First up, in a new Python file called app.py, we're going to import some initial dependencies. OS is going to be used to set the API key. From langchain.llms, we'll import the OpenAI service. This will be the core LLM we use. We'll also bring in Streamlit while we're at it. Once that's done, we can set the API key and create the OpenAI service as an LLM. If we wanted to, we could tune the temperature depending on how creative or objective we wanted our responses. We also need somewhere to pass the prompt. We can use the Streamlit.TextInput element to do this. If our user hits enter, we need a way to send the prompt to the LLM. Let's throw in an if statement here and write out the text to the screen using streamlit.write. To start the app it's relatively straightforward, just write streamlit run app.py or whatever your python file is named. Side note, all this code is going to be available in the description below so you can give it a crack yourself. This gives us a baseline app and uses open AI to generate responses. But there's one key issue here, we haven't gone and used our own documents yet. Let's fix that but before we do, a word from our sponsors. Me. If you'd like to get up and running with Python for machine learning, head over to go.coursesfromnick.com forward slash Python, where you can take my end-to-end tech fundamentals course. Or if you want to dive straight into the deep end, you can check out my full stack machine learning course at this link here. And use YouTube 50 to get 50% off right now. Back to the video. We need At the top of our script, we're going to bring in PyPDFLoader from langchain.documentloaders and Chroma from langchain.vectorstores. PyPDFLoader is used to load and pass a PDF into memory. Chroma is a vector store that is critical to using your own documents. What actually happens, as far as I know, is your document is tokenized and loaded into Chroma for querying later on. This allows you to perform similarity search using similarity metrics like Euclidean distance, similarity metrics like Euclidean distance. Also similar to how Pinecone works. We'll do this in a sec. All right, time to load up our document. Using the PyPDF loader class, we're going to upload a banking annual report. I've got the files stored in the same place as my Python script, so I can just pass the name of the document direct to the class. If you wanted to use your own document, you could sub in the name of the file here. In this case, it's got to be a PDF, be a PDF but there are other loaders available inside of Langchain. We can then load the document into Chroma using the from documents method. To do this pass the pages from the PDF to the loader method. And then back to that similarity search. Using a Streamlit expander class we can search through the document in natural text and render the results to the screen. Say we ask about the performance of the bank. Chroma will return the relevant passages from the document loaded. These will eventually be passed through to the Langchain agent to generate a human-like response based on that context. But we don't need to do this manually. We can use the VectorStore agent from Langchain. This will package it all up pretty nicely. Last set of imports, home stretch now. First up, we'll import create VectorStore agent, VectorStore toolkit, and VectorStore info from Langchain.agents.agent toolkits. The vector store info class as far as I can tell just provides context about the store aka the pdf that we're going to pass through to our llm agent. You just pass your name, description and chroma store to that specific class. Then the magic happens. We can pass the vector store info wrapper to vector store toolkit. This makes the pdf available as a tool to langchain similar to how we use the wikipedia toolkit in the langchain crash course video. And last but not least bring it it together with the VectorStore agent. This is the most critical part. So the agent creator class will pass through our original OpenAI LLM service and the VectorStore toolkit. This packages it all up nicely and will in effect give our large language model agent access to our PDF. Rather than just outputting responses from the LLM like we had previously, we can now use the agent and run the prompt through it and again write out the responses to the screen this allows us to ask things like what was the net profit of the company with the response that we're getting back from our gpt investment banker being 4 706 million or 56 on the prior year which just so happens to match note 6 on earnings per share in the notes to the financial statements. What initiatives did the bank take towards sustainability? Our model is calling out the Net Zero Asset Managers Initiative and the Net Zero Banking Alliance. This is pretty closely tied to the governance section in the financial statement as well. And last but not least, summarize the financial performance of the bank. The model calls out net profit after taxes increasing by 56% compared to the prior year and EPS increasing by 51%. This just so happens to match the letter from the chair of the board of the remuneration committee calling out those exact same NPAT and EPS numbers. The document uses roughly 300 pages in length so documents can take a little while to return a response. That being said in 45 lines of code we've got our very own personal investment banker. Not too shabby. If you want to check out the lane crash course video I did go and click here.\",\n",
       " 'chunks': [{'timestamp': (0.08, 2.36),\n",
       "   'text': ' This is an AI agent doing financial analysis.'},\n",
       "  {'timestamp': (2.36, 5.84),\n",
       "   'text': ' I used it to build an LLM-Langchain app that could analyze financial statements.'},\n",
       "  {'timestamp': (5.84, 9.92),\n",
       "   'text': \" In fact, it's doing it without fine-tuning on this 300-page banking annual report right\"},\n",
       "  {'timestamp': (9.92, 10.92), 'text': ' now.'},\n",
       "  {'timestamp': (10.92, 12.04),\n",
       "   'text': \" I'm going to teach you how to build it in 5 minutes.\"},\n",
       "  {'timestamp': (12.04, 15.84),\n",
       "   'text': ' I went down the rabbit hole to discover how to use your own documents to build AI-powered'},\n",
       "  {'timestamp': (15.84, 18.4),\n",
       "   'text': ' large language model apps with Langchain.'},\n",
       "  {'timestamp': (18.4, 25.8),\n",
       "   'text': \" And the results speak for themselves. But first of all, why should we even bother trying to use our own documents? We've already built our own large language model app with Streamlit.\"},\n",
       "  {'timestamp': (25.8, 26.8), 'text': ' Why the need for docos?'},\n",
       "  {'timestamp': (26.8, 30.74),\n",
       "   'text': ' Well, it gives you the ultimate flexibility to leverage large language models for tasks'},\n",
       "  {'timestamp': (30.74, 32.68), 'text': ' that are specific to you.'},\n",
       "  {'timestamp': (32.68, 34.26),\n",
       "   'text': ' Provide meeting minutes and create a summary.'},\n",
       "  {'timestamp': (34.26, 37.18),\n",
       "   'text': ' You could upload an assignment outline and have a response generated.'},\n",
       "  {'timestamp': (37.18, 50.24),\n",
       "   'text': \" Upload an annual report and have a large language model do the financial analysis for you. In fact, that's what we're going to be doing we're going to be building our very own personal ai powered investment banker using streamlit and our own annual report to get\"},\n",
       "  {'timestamp': (50.24, 54.88),\n",
       "   'text': ' started we first need to install some dependencies namely langchain openai streamlit tick token'},\n",
       "  {'timestamp': (54.88, 60.64),\n",
       "   'text': \" chroma db pi pdf and crypto dome while we're at it we're going to need to get an openai api key it's\"},\n",
       "  {'timestamp': (60.64, 65.22),\n",
       "   'text': ' optional to use the openai service you could substitute substitute it out for a Lama CPP or one'},\n",
       "  {'timestamp': (65.22, 68.46),\n",
       "   'text': ' of the Hugging Face hosted models. If you want a vid on that, let me know. Time to create our app.'},\n",
       "  {'timestamp': (68.5, 72.36),\n",
       "   'text': \" First up, in a new Python file called app.py, we're going to import some initial dependencies.\"},\n",
       "  {'timestamp': (72.6, 78.18),\n",
       "   'text': \" OS is going to be used to set the API key. From langchain.llms, we'll import the OpenAI service.\"},\n",
       "  {'timestamp': (86.12, 89.96),\n",
       "   'text': \" This will be the core LLM we use. We'll also bring in Streamlit while we're at it. Once that's done, we can set the API key and create the OpenAI service as an LLM. If we wanted to, we could tune the temperature depending on how creative or objective we\"},\n",
       "  {'timestamp': (89.96, 90.96), 'text': ' wanted our responses.'},\n",
       "  {'timestamp': (90.96, 93.1),\n",
       "   'text': ' We also need somewhere to pass the prompt.'},\n",
       "  {'timestamp': (93.1, 95.68),\n",
       "   'text': ' We can use the Streamlit.TextInput element to do this.'},\n",
       "  {'timestamp': (95.68, 106.94),\n",
       "   'text': \" If our user hits enter, we need a way to send the prompt to the LLM. Let's throw in an if statement here and write out the text to the screen using streamlit.write. To start the app it's relatively straightforward, just write streamlit run app.py or whatever\"},\n",
       "  {'timestamp': (106.94, 108.32), 'text': ' your python file is named.'},\n",
       "  {'timestamp': (108.32, 111.36),\n",
       "   'text': ' Side note, all this code is going to be available in the description below so you can give it'},\n",
       "  {'timestamp': (111.36, 112.36), 'text': ' a crack yourself.'},\n",
       "  {'timestamp': (112.36, 115.84),\n",
       "   'text': ' This gives us a baseline app and uses open AI to generate responses.'},\n",
       "  {'timestamp': (115.84, 119.76),\n",
       "   'text': \" But there's one key issue here, we haven't gone and used our own documents yet.\"},\n",
       "  {'timestamp': (119.76, 127.5),\n",
       "   'text': \" Let's fix that but before we do, a word from our sponsors. Me. If you'd like to get up and running with Python for machine learning, head over to go.coursesfromnick.com forward slash Python,\"},\n",
       "  {'timestamp': (127.5, 130.0),\n",
       "   'text': ' where you can take my end-to-end tech fundamentals course.'},\n",
       "  {'timestamp': (130.0, 132.18),\n",
       "   'text': ' Or if you want to dive straight into the deep end,'},\n",
       "  {'timestamp': (132.18, 134.38),\n",
       "   'text': ' you can check out my full stack machine learning course'},\n",
       "  {'timestamp': (134.38, 135.34), 'text': ' at this link here.'},\n",
       "  {'timestamp': (135.34, 137.8),\n",
       "   'text': ' And use YouTube 50 to get 50% off right now.'},\n",
       "  {'timestamp': (137.8, 146.8),\n",
       "   'text': \" Back to the video. We need At the top of our script, we're going to bring in PyPDFLoader from langchain.documentloaders and Chroma from langchain.vectorstores.\"},\n",
       "  {'timestamp': (147.22, 149.58),\n",
       "   'text': ' PyPDFLoader is used to load and pass a PDF into memory.'},\n",
       "  {'timestamp': (149.84, 153.14),\n",
       "   'text': ' Chroma is a vector store that is critical to using your own documents.'},\n",
       "  {'timestamp': (153.24, 156.68),\n",
       "   'text': ' What actually happens, as far as I know, is your document is tokenized and loaded into'},\n",
       "  {'timestamp': (156.68, 157.98), 'text': ' Chroma for querying later on.'},\n",
       "  {'timestamp': (158.04, 162.08),\n",
       "   'text': ' This allows you to perform similarity search using similarity metrics like Euclidean distance,'},\n",
       "  {'timestamp': (165.06, 168.44),\n",
       "   'text': \" similarity metrics like Euclidean distance. Also similar to how Pinecone works. We'll do this in a sec. All right, time to load up our document. Using the PyPDF loader class, we're going to\"},\n",
       "  {'timestamp': (168.44, 172.42),\n",
       "   'text': \" upload a banking annual report. I've got the files stored in the same place as my Python script,\"},\n",
       "  {'timestamp': (172.52, 176.72),\n",
       "   'text': ' so I can just pass the name of the document direct to the class. If you wanted to use your'},\n",
       "  {'timestamp': (176.72, 180.58),\n",
       "   'text': \" own document, you could sub in the name of the file here. In this case, it's got to be a PDF,\"},\n",
       "  {'timestamp': (184.0, 189.2),\n",
       "   'text': ' be a PDF but there are other loaders available inside of Langchain. We can then load the document into Chroma using the from documents method. To do this pass the pages from the PDF to the'},\n",
       "  {'timestamp': (189.2, 193.2),\n",
       "   'text': ' loader method. And then back to that similarity search. Using a Streamlit expander class we can'},\n",
       "  {'timestamp': (193.2, 197.44),\n",
       "   'text': ' search through the document in natural text and render the results to the screen. Say we ask about'},\n",
       "  {'timestamp': (197.44, 206.88),\n",
       "   'text': ' the performance of the bank. Chroma will return the relevant passages from the document loaded. These will eventually be passed through to the Langchain agent to generate a human-like response'},\n",
       "  {'timestamp': (206.88, 208.18), 'text': ' based on that context.'},\n",
       "  {'timestamp': (208.18, 209.56),\n",
       "   'text': \" But we don't need to do this manually.\"},\n",
       "  {'timestamp': (209.56, 211.9),\n",
       "   'text': ' We can use the VectorStore agent from Langchain.'},\n",
       "  {'timestamp': (211.9, 213.56),\n",
       "   'text': ' This will package it all up pretty nicely.'},\n",
       "  {'timestamp': (213.56, 215.12),\n",
       "   'text': ' Last set of imports, home stretch now.'},\n",
       "  {'timestamp': (215.12, 217.28),\n",
       "   'text': \" First up, we'll import create VectorStore agent,\"},\n",
       "  {'timestamp': (217.28, 225.48),\n",
       "   'text': ' VectorStore toolkit, and VectorStore info from Langchain.agents.agent toolkits. The vector store info class as far as I can tell just provides context about the'},\n",
       "  {'timestamp': (225.48, 229.64),\n",
       "   'text': \" store aka the pdf that we're going to pass through to our llm agent. You just pass your name,\"},\n",
       "  {'timestamp': (229.74, 234.82),\n",
       "   'text': ' description and chroma store to that specific class. Then the magic happens. We can pass the'},\n",
       "  {'timestamp': (234.82, 239.46),\n",
       "   'text': ' vector store info wrapper to vector store toolkit. This makes the pdf available as a tool to'},\n",
       "  {'timestamp': (239.46, 243.6),\n",
       "   'text': ' langchain similar to how we use the wikipedia toolkit in the langchain crash course video.'},\n",
       "  {'timestamp': (246.0, 246.16),\n",
       "   'text': ' And last but not least bring it it together with the VectorStore agent.'},\n",
       "  {'timestamp': (247.46, 247.88), 'text': ' This is the most critical part.'},\n",
       "  {'timestamp': (251.7, 253.32),\n",
       "   'text': ' So the agent creator class will pass through our original OpenAI LLM service and the VectorStore toolkit.'},\n",
       "  {'timestamp': (253.68, 256.36),\n",
       "   'text': ' This packages it all up nicely and will in effect'},\n",
       "  {'timestamp': (256.36, 259.26),\n",
       "   'text': ' give our large language model agent access to our PDF.'},\n",
       "  {'timestamp': (259.54, 262.64),\n",
       "   'text': ' Rather than just outputting responses from the LLM like we had previously,'},\n",
       "  {'timestamp': (269.36, 273.12),\n",
       "   'text': \" we can now use the agent and run the prompt through it and again write out the responses to the screen this allows us to ask things like what was the net profit of the company with the response that we're getting back from our gpt\"},\n",
       "  {'timestamp': (273.12, 280.16),\n",
       "   'text': ' investment banker being 4 706 million or 56 on the prior year which just so happens to match note 6'},\n",
       "  {'timestamp': (280.16, 286.22),\n",
       "   'text': ' on earnings per share in the notes to the financial statements. What initiatives did the bank take towards sustainability?'},\n",
       "  {'timestamp': (286.66, 291.28),\n",
       "   'text': ' Our model is calling out the Net Zero Asset Managers Initiative and the Net Zero Banking'},\n",
       "  {'timestamp': (291.28, 294.72),\n",
       "   'text': ' Alliance. This is pretty closely tied to the governance section in the financial statement'},\n",
       "  {'timestamp': (294.72, 298.0),\n",
       "   'text': ' as well. And last but not least, summarize the financial performance of the bank.'},\n",
       "  {'timestamp': (298.64, 303.62),\n",
       "   'text': ' The model calls out net profit after taxes increasing by 56% compared to the prior year'},\n",
       "  {'timestamp': (303.62, 305.7), 'text': ' and EPS increasing by 51%.'},\n",
       "  {'timestamp': (305.7, 309.4),\n",
       "   'text': ' This just so happens to match the letter from the chair of the board of the remuneration committee'},\n",
       "  {'timestamp': (309.4, 315.82),\n",
       "   'text': ' calling out those exact same NPAT and EPS numbers. The document uses roughly 300 pages in length so'},\n",
       "  {'timestamp': (315.82, 319.96),\n",
       "   'text': ' documents can take a little while to return a response. That being said in 45 lines of code'},\n",
       "  {'timestamp': (319.96, 323.74),\n",
       "   'text': \" we've got our very own personal investment banker. Not too shabby. If you want to check out the lane\"},\n",
       "  {'timestamp': (323.74, 325.82),\n",
       "   'text': ' crash course video I did go and click here.'}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert(r'F:\\works\\A-important\\A-neurals\\IEEE-PaperGen\\I built a GPT Investment Banker using this 312 PAGE document.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
