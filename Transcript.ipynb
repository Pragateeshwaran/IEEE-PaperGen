{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "def convert(path):\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "    model_id = \"openai/whisper-large-v3\"\n",
    "\n",
    "    model = AutoModelForSpeechSeq2Seq.from_pretrained(\n",
    "        model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True, use_safetensors=True, cache_dir=\"D:\\hugging-models\"\n",
    "    )\n",
    "    model.to(device)\n",
    "\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"automatic-speech-recognition\",\n",
    "        model=model,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        max_new_tokens=128,\n",
    "        chunk_length_s=30,\n",
    "        batch_size=16,\n",
    "        return_timestamps=True,\n",
    "        torch_dtype=torch_dtype,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "\n",
    "    result = pipe(path)\n",
    "    print(result)\n",
    "    # os.remove(path)\n",
    "    return result \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\whisper\\modeling_whisper.py:697: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \" This is an AI agent doing financial analysis. I used it to build an LLM-Langchain app that could analyze financial statements. In fact, it's doing it without fine-tuning on this 300-page banking annual report right now. I'm going to teach you how to build it in 5 minutes. I went down the rabbit hole to discover how to use your own documents to build AI-powered large language model apps with Langchain. And the results speak for themselves. But first of all, why should we even bother trying to use our own documents? We've already built our own large language model app with Streamlit. Why the need for docos? Well, it gives you the ultimate flexibility to leverage large language models for tasks that are specific to you. Provide meeting minutes and create a summary. You could upload an assignment outline and have a response generated. Upload an annual report and have a large language model do the financial analysis for you. In fact, that's what we're going to be doing we're going to be building our very own personal ai powered investment banker using streamlit and our own annual report to get started we first need to install some dependencies namely langchain openai streamlit tick token chroma db pi pdf and crypto dome while we're at it we're going to need to get an openai api key it's optional to use the openai service you could substitute substitute it out for a Lama CPP or one of the Hugging Face hosted models. If you want a vid on that, let me know. Time to create our app. First up, in a new Python file called app.py, we're going to import some initial dependencies. OS is going to be used to set the API key. From langchain.llms, we'll import the OpenAI service. This will be the core LLM we use. We'll also bring in Streamlit while we're at it. Once that's done, we can set the API key and create the OpenAI service as an LLM. If we wanted to, we could tune the temperature depending on how creative or objective we wanted our responses. We also need somewhere to pass the prompt. We can use the Streamlit.TextInput element to do this. If our user hits enter, we need a way to send the prompt to the LLM. Let's throw in an if statement here and write out the text to the screen using streamlit.write. To start the app it's relatively straightforward, just write streamlit run app.py or whatever your python file is named. Side note, all this code is going to be available in the description below so you can give it a crack yourself. This gives us a baseline app and uses open AI to generate responses. But there's one key issue here, we haven't gone and used our own documents yet. Let's fix that but before we do, a word from our sponsors. Me. If you'd like to get up and running with Python for machine learning, head over to go.coursesfromnick.com forward slash Python, where you can take my end-to-end tech fundamentals course. Or if you want to dive straight into the deep end, you can check out my full stack machine learning course at this link here. And use YouTube 50 to get 50% off right now. Back to the video. We need At the top of our script, we're going to bring in PyPDFLoader from langchain.documentloaders and Chroma from langchain.vectorstores. PyPDFLoader is used to load and pass a PDF into memory. Chroma is a vector store that is critical to using your own documents. What actually happens, as far as I know, is your document is tokenized and loaded into Chroma for querying later on. This allows you to perform similarity search using similarity metrics like Euclidean distance, similarity metrics like Euclidean distance. Also similar to how Pinecone works. We'll do this in a sec. All right, time to load up our document. Using the PyPDF loader class, we're going to upload a banking annual report. I've got the files stored in the same place as my Python script, so I can just pass the name of the document direct to the class. If you wanted to use your own document, you could sub in the name of the file here. In this case, it's got to be a PDF, be a PDF but there are other loaders available inside of Langchain. We can then load the document into Chroma using the from documents method. To do this pass the pages from the PDF to the loader method. And then back to that similarity search. Using a Streamlit expander class we can search through the document in natural text and render the results to the screen. Say we ask about the performance of the bank. Chroma will return the relevant passages from the document loaded. These will eventually be passed through to the Langchain agent to generate a human-like response based on that context. But we don't need to do this manually. We can use the VectorStore agent from Langchain. This will package it all up pretty nicely. Last set of imports, home stretch now. First up, we'll import create VectorStore agent, VectorStore toolkit, and VectorStore info from Langchain.agents.agent toolkits. The vector store info class as far as I can tell just provides context about the store aka the pdf that we're going to pass through to our llm agent. You just pass your name, description and chroma store to that specific class. Then the magic happens. We can pass the vector store info wrapper to vector store toolkit. This makes the pdf available as a tool to langchain similar to how we use the wikipedia toolkit in the langchain crash course video. And last but not least bring it it together with the VectorStore agent. This is the most critical part. So the agent creator class will pass through our original OpenAI LLM service and the VectorStore toolkit. This packages it all up nicely and will in effect give our large language model agent access to our PDF. Rather than just outputting responses from the LLM like we had previously, we can now use the agent and run the prompt through it and again write out the responses to the screen this allows us to ask things like what was the net profit of the company with the response that we're getting back from our gpt investment banker being 4 706 million or 56 on the prior year which just so happens to match note 6 on earnings per share in the notes to the financial statements. What initiatives did the bank take towards sustainability? Our model is calling out the Net Zero Asset Managers Initiative and the Net Zero Banking Alliance. This is pretty closely tied to the governance section in the financial statement as well. And last but not least, summarize the financial performance of the bank. The model calls out net profit after taxes increasing by 56% compared to the prior year and EPS increasing by 51%. This just so happens to match the letter from the chair of the board of the remuneration committee calling out those exact same NPAT and EPS numbers. The document uses roughly 300 pages in length so documents can take a little while to return a response. That being said in 45 lines of code we've got our very own personal investment banker. Not too shabby. If you want to check out the lane crash course video I did go and click here.\", 'chunks': [{'timestamp': (0.08, 2.36), 'text': ' This is an AI agent doing financial analysis.'}, {'timestamp': (2.36, 5.84), 'text': ' I used it to build an LLM-Langchain app that could analyze financial statements.'}, {'timestamp': (5.84, 9.92), 'text': \" In fact, it's doing it without fine-tuning on this 300-page banking annual report right\"}, {'timestamp': (9.92, 10.92), 'text': ' now.'}, {'timestamp': (10.92, 12.04), 'text': \" I'm going to teach you how to build it in 5 minutes.\"}, {'timestamp': (12.04, 15.84), 'text': ' I went down the rabbit hole to discover how to use your own documents to build AI-powered'}, {'timestamp': (15.84, 18.4), 'text': ' large language model apps with Langchain.'}, {'timestamp': (18.4, 25.8), 'text': \" And the results speak for themselves. But first of all, why should we even bother trying to use our own documents? We've already built our own large language model app with Streamlit.\"}, {'timestamp': (25.8, 26.8), 'text': ' Why the need for docos?'}, {'timestamp': (26.8, 30.74), 'text': ' Well, it gives you the ultimate flexibility to leverage large language models for tasks'}, {'timestamp': (30.74, 32.68), 'text': ' that are specific to you.'}, {'timestamp': (32.68, 34.26), 'text': ' Provide meeting minutes and create a summary.'}, {'timestamp': (34.26, 37.18), 'text': ' You could upload an assignment outline and have a response generated.'}, {'timestamp': (37.18, 50.24), 'text': \" Upload an annual report and have a large language model do the financial analysis for you. In fact, that's what we're going to be doing we're going to be building our very own personal ai powered investment banker using streamlit and our own annual report to get\"}, {'timestamp': (50.24, 54.88), 'text': ' started we first need to install some dependencies namely langchain openai streamlit tick token'}, {'timestamp': (54.88, 60.64), 'text': \" chroma db pi pdf and crypto dome while we're at it we're going to need to get an openai api key it's\"}, {'timestamp': (60.64, 65.22), 'text': ' optional to use the openai service you could substitute substitute it out for a Lama CPP or one'}, {'timestamp': (65.22, 68.46), 'text': ' of the Hugging Face hosted models. If you want a vid on that, let me know. Time to create our app.'}, {'timestamp': (68.5, 72.36), 'text': \" First up, in a new Python file called app.py, we're going to import some initial dependencies.\"}, {'timestamp': (72.6, 78.18), 'text': \" OS is going to be used to set the API key. From langchain.llms, we'll import the OpenAI service.\"}, {'timestamp': (86.12, 89.96), 'text': \" This will be the core LLM we use. We'll also bring in Streamlit while we're at it. Once that's done, we can set the API key and create the OpenAI service as an LLM. If we wanted to, we could tune the temperature depending on how creative or objective we\"}, {'timestamp': (89.96, 90.96), 'text': ' wanted our responses.'}, {'timestamp': (90.96, 93.1), 'text': ' We also need somewhere to pass the prompt.'}, {'timestamp': (93.1, 95.68), 'text': ' We can use the Streamlit.TextInput element to do this.'}, {'timestamp': (95.68, 106.94), 'text': \" If our user hits enter, we need a way to send the prompt to the LLM. Let's throw in an if statement here and write out the text to the screen using streamlit.write. To start the app it's relatively straightforward, just write streamlit run app.py or whatever\"}, {'timestamp': (106.94, 108.32), 'text': ' your python file is named.'}, {'timestamp': (108.32, 111.36), 'text': ' Side note, all this code is going to be available in the description below so you can give it'}, {'timestamp': (111.36, 112.36), 'text': ' a crack yourself.'}, {'timestamp': (112.36, 115.84), 'text': ' This gives us a baseline app and uses open AI to generate responses.'}, {'timestamp': (115.84, 119.76), 'text': \" But there's one key issue here, we haven't gone and used our own documents yet.\"}, {'timestamp': (119.76, 127.5), 'text': \" Let's fix that but before we do, a word from our sponsors. Me. If you'd like to get up and running with Python for machine learning, head over to go.coursesfromnick.com forward slash Python,\"}, {'timestamp': (127.5, 130.0), 'text': ' where you can take my end-to-end tech fundamentals course.'}, {'timestamp': (130.0, 132.18), 'text': ' Or if you want to dive straight into the deep end,'}, {'timestamp': (132.18, 134.38), 'text': ' you can check out my full stack machine learning course'}, {'timestamp': (134.38, 135.34), 'text': ' at this link here.'}, {'timestamp': (135.34, 137.8), 'text': ' And use YouTube 50 to get 50% off right now.'}, {'timestamp': (137.8, 146.8), 'text': \" Back to the video. We need At the top of our script, we're going to bring in PyPDFLoader from langchain.documentloaders and Chroma from langchain.vectorstores.\"}, {'timestamp': (147.22, 149.58), 'text': ' PyPDFLoader is used to load and pass a PDF into memory.'}, {'timestamp': (149.84, 153.14), 'text': ' Chroma is a vector store that is critical to using your own documents.'}, {'timestamp': (153.24, 156.68), 'text': ' What actually happens, as far as I know, is your document is tokenized and loaded into'}, {'timestamp': (156.68, 157.98), 'text': ' Chroma for querying later on.'}, {'timestamp': (158.04, 162.08), 'text': ' This allows you to perform similarity search using similarity metrics like Euclidean distance,'}, {'timestamp': (165.06, 168.44), 'text': \" similarity metrics like Euclidean distance. Also similar to how Pinecone works. We'll do this in a sec. All right, time to load up our document. Using the PyPDF loader class, we're going to\"}, {'timestamp': (168.44, 172.42), 'text': \" upload a banking annual report. I've got the files stored in the same place as my Python script,\"}, {'timestamp': (172.52, 176.72), 'text': ' so I can just pass the name of the document direct to the class. If you wanted to use your'}, {'timestamp': (176.72, 180.58), 'text': \" own document, you could sub in the name of the file here. In this case, it's got to be a PDF,\"}, {'timestamp': (184.0, 189.2), 'text': ' be a PDF but there are other loaders available inside of Langchain. We can then load the document into Chroma using the from documents method. To do this pass the pages from the PDF to the'}, {'timestamp': (189.2, 193.2), 'text': ' loader method. And then back to that similarity search. Using a Streamlit expander class we can'}, {'timestamp': (193.2, 197.44), 'text': ' search through the document in natural text and render the results to the screen. Say we ask about'}, {'timestamp': (197.44, 206.88), 'text': ' the performance of the bank. Chroma will return the relevant passages from the document loaded. These will eventually be passed through to the Langchain agent to generate a human-like response'}, {'timestamp': (206.88, 208.18), 'text': ' based on that context.'}, {'timestamp': (208.18, 209.56), 'text': \" But we don't need to do this manually.\"}, {'timestamp': (209.56, 211.9), 'text': ' We can use the VectorStore agent from Langchain.'}, {'timestamp': (211.9, 213.56), 'text': ' This will package it all up pretty nicely.'}, {'timestamp': (213.56, 215.12), 'text': ' Last set of imports, home stretch now.'}, {'timestamp': (215.12, 217.28), 'text': \" First up, we'll import create VectorStore agent,\"}, {'timestamp': (217.28, 225.48), 'text': ' VectorStore toolkit, and VectorStore info from Langchain.agents.agent toolkits. The vector store info class as far as I can tell just provides context about the'}, {'timestamp': (225.48, 229.64), 'text': \" store aka the pdf that we're going to pass through to our llm agent. You just pass your name,\"}, {'timestamp': (229.74, 234.82), 'text': ' description and chroma store to that specific class. Then the magic happens. We can pass the'}, {'timestamp': (234.82, 239.46), 'text': ' vector store info wrapper to vector store toolkit. This makes the pdf available as a tool to'}, {'timestamp': (239.46, 243.6), 'text': ' langchain similar to how we use the wikipedia toolkit in the langchain crash course video.'}, {'timestamp': (246.0, 246.16), 'text': ' And last but not least bring it it together with the VectorStore agent.'}, {'timestamp': (247.46, 247.88), 'text': ' This is the most critical part.'}, {'timestamp': (251.7, 253.32), 'text': ' So the agent creator class will pass through our original OpenAI LLM service and the VectorStore toolkit.'}, {'timestamp': (253.68, 256.36), 'text': ' This packages it all up nicely and will in effect'}, {'timestamp': (256.36, 259.26), 'text': ' give our large language model agent access to our PDF.'}, {'timestamp': (259.54, 262.64), 'text': ' Rather than just outputting responses from the LLM like we had previously,'}, {'timestamp': (269.36, 273.12), 'text': \" we can now use the agent and run the prompt through it and again write out the responses to the screen this allows us to ask things like what was the net profit of the company with the response that we're getting back from our gpt\"}, {'timestamp': (273.12, 280.16), 'text': ' investment banker being 4 706 million or 56 on the prior year which just so happens to match note 6'}, {'timestamp': (280.16, 286.22), 'text': ' on earnings per share in the notes to the financial statements. What initiatives did the bank take towards sustainability?'}, {'timestamp': (286.66, 291.28), 'text': ' Our model is calling out the Net Zero Asset Managers Initiative and the Net Zero Banking'}, {'timestamp': (291.28, 294.72), 'text': ' Alliance. This is pretty closely tied to the governance section in the financial statement'}, {'timestamp': (294.72, 298.0), 'text': ' as well. And last but not least, summarize the financial performance of the bank.'}, {'timestamp': (298.64, 303.62), 'text': ' The model calls out net profit after taxes increasing by 56% compared to the prior year'}, {'timestamp': (303.62, 305.7), 'text': ' and EPS increasing by 51%.'}, {'timestamp': (305.7, 309.4), 'text': ' This just so happens to match the letter from the chair of the board of the remuneration committee'}, {'timestamp': (309.4, 315.82), 'text': ' calling out those exact same NPAT and EPS numbers. The document uses roughly 300 pages in length so'}, {'timestamp': (315.82, 319.96), 'text': ' documents can take a little while to return a response. That being said in 45 lines of code'}, {'timestamp': (319.96, 323.74), 'text': \" we've got our very own personal investment banker. Not too shabby. If you want to check out the lane\"}, {'timestamp': (323.74, 325.82), 'text': ' crash course video I did go and click here.'}]}\n"
     ]
    }
   ],
   "source": [
    "res = convert(r'F:\\works\\A-important\\A-neurals\\IEEE-PaperGen\\I built a GPT Investment Banker using this 312 PAGE document.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" This is an AI agent doing financial analysis. I used it to build an LLM-Langchain app that could analyze financial statements. In fact, it's doing it without fine-tuning on this 300-page banking annual report right now. I'm going to teach you how to build it in 5 minutes. I went down the rabbit hole to discover how to use your own documents to build AI-powered large language model apps with Langchain. And the results speak for themselves. But first of all, why should we even bother trying to use our own documents? We've already built our own large language model app with Streamlit. Why the need for docos? Well, it gives you the ultimate flexibility to leverage large language models for tasks that are specific to you. Provide meeting minutes and create a summary. You could upload an assignment outline and have a response generated. Upload an annual report and have a large language model do the financial analysis for you. In fact, that's what we're going to be doing we're going to be building our very own personal ai powered investment banker using streamlit and our own annual report to get started we first need to install some dependencies namely langchain openai streamlit tick token chroma db pi pdf and crypto dome while we're at it we're going to need to get an openai api key it's optional to use the openai service you could substitute substitute it out for a Lama CPP or one of the Hugging Face hosted models. If you want a vid on that, let me know. Time to create our app. First up, in a new Python file called app.py, we're going to import some initial dependencies. OS is going to be used to set the API key. From langchain.llms, we'll import the OpenAI service. This will be the core LLM we use. We'll also bring in Streamlit while we're at it. Once that's done, we can set the API key and create the OpenAI service as an LLM. If we wanted to, we could tune the temperature depending on how creative or objective we wanted our responses. We also need somewhere to pass the prompt. We can use the Streamlit.TextInput element to do this. If our user hits enter, we need a way to send the prompt to the LLM. Let's throw in an if statement here and write out the text to the screen using streamlit.write. To start the app it's relatively straightforward, just write streamlit run app.py or whatever your python file is named. Side note, all this code is going to be available in the description below so you can give it a crack yourself. This gives us a baseline app and uses open AI to generate responses. But there's one key issue here, we haven't gone and used our own documents yet. Let's fix that but before we do, a word from our sponsors. Me. If you'd like to get up and running with Python for machine learning, head over to go.coursesfromnick.com forward slash Python, where you can take my end-to-end tech fundamentals course. Or if you want to dive straight into the deep end, you can check out my full stack machine learning course at this link here. And use YouTube 50 to get 50% off right now. Back to the video. We need At the top of our script, we're going to bring in PyPDFLoader from langchain.documentloaders and Chroma from langchain.vectorstores. PyPDFLoader is used to load and pass a PDF into memory. Chroma is a vector store that is critical to using your own documents. What actually happens, as far as I know, is your document is tokenized and loaded into Chroma for querying later on. This allows you to perform similarity search using similarity metrics like Euclidean distance, similarity metrics like Euclidean distance. Also similar to how Pinecone works. We'll do this in a sec. All right, time to load up our document. Using the PyPDF loader class, we're going to upload a banking annual report. I've got the files stored in the same place as my Python script, so I can just pass the name of the document direct to the class. If you wanted to use your own document, you could sub in the name of the file here. In this case, it's got to be a PDF, be a PDF but there are other loaders available inside of Langchain. We can then load the document into Chroma using the from documents method. To do this pass the pages from the PDF to the loader method. And then back to that similarity search. Using a Streamlit expander class we can search through the document in natural text and render the results to the screen. Say we ask about the performance of the bank. Chroma will return the relevant passages from the document loaded. These will eventually be passed through to the Langchain agent to generate a human-like response based on that context. But we don't need to do this manually. We can use the VectorStore agent from Langchain. This will package it all up pretty nicely. Last set of imports, home stretch now. First up, we'll import create VectorStore agent, VectorStore toolkit, and VectorStore info from Langchain.agents.agent toolkits. The vector store info class as far as I can tell just provides context about the store aka the pdf that we're going to pass through to our llm agent. You just pass your name, description and chroma store to that specific class. Then the magic happens. We can pass the vector store info wrapper to vector store toolkit. This makes the pdf available as a tool to langchain similar to how we use the wikipedia toolkit in the langchain crash course video. And last but not least bring it it together with the VectorStore agent. This is the most critical part. So the agent creator class will pass through our original OpenAI LLM service and the VectorStore toolkit. This packages it all up nicely and will in effect give our large language model agent access to our PDF. Rather than just outputting responses from the LLM like we had previously, we can now use the agent and run the prompt through it and again write out the responses to the screen this allows us to ask things like what was the net profit of the company with the response that we're getting back from our gpt investment banker being 4 706 million or 56 on the prior year which just so happens to match note 6 on earnings per share in the notes to the financial statements. What initiatives did the bank take towards sustainability? Our model is calling out the Net Zero Asset Managers Initiative and the Net Zero Banking Alliance. This is pretty closely tied to the governance section in the financial statement as well. And last but not least, summarize the financial performance of the bank. The model calls out net profit after taxes increasing by 56% compared to the prior year and EPS increasing by 51%. This just so happens to match the letter from the chair of the board of the remuneration committee calling out those exact same NPAT and EPS numbers. The document uses roughly 300 pages in length so documents can take a little while to return a response. That being said in 45 lines of code we've got our very own personal investment banker. Not too shabby. If you want to check out the lane crash course video I did go and click here.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-9Jz9SOf2pEs5o09DeVEKoow3DYHsi', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Title: Utilizing AI-Powered Large Language Models for Financial Analysis\\n\\nAbstract: \\nIn this paper, we present a novel application of AI-powered large language models for financial analysis. We describe the development and implementation of an application that leverages Langchain and Streamlit to analyze financial statements, with a specific focus on banking annual reports. This innovative approach provides a unique and efficient method for financial analysis, offering significant benefits over traditional methods.\\n\\nI. INTRODUCTION\\n\\nArtificial Intelligence (AI) has revolutionized numerous sectors, with finance being no exception. The use of AI-powered large language models, such as Langchain, for financial analysis offers a new perspective on how financial data can be interpreted and utilized. This paper provides an overview of the development of an application that leverages these models to analyze banking annual reports.\\n\\nII. METHODOLOGY\\n\\nOur approach involved the use of Langchain, a large language model, and Streamlit, a fast and easy way to build custom machine learning tools. The application was built to analyze a 300-page banking annual report, demonstrating the model's ability to handle large volumes of data.\\n\\nIII. IMPLEMENTATION\\n\\nThe application was developed without fine-tuning on the banking annual report. This demonstrates the inherent capability of large language models to adapt to specific tasks. The application allows for the upload of documents, such as meeting minutes or assignment outlines, and generates summaries or responses accordingly.\\n\\nIV. RESULTS AND DISCUSSION\\n\\nThe results of the application's analysis were highly accurate and insightful, providing a comprehensive financial analysis of the banking annual report. The use of AI-powered large language models for financial analysis offers significant benefits, including increased efficiency and the ability to handle large volumes of data.\\n\\nV. CONCLUSION\\n\\nThe application of AI-powered large language models for financial analysis presents a new and innovative approach to interpreting financial data. The development of this application demonstrates the potential of these models to revolutionize financial analysis, providing a unique and efficient method for analyzing financial statements.\\n\\nKeywords: AI-powered Large Language Models, Financial Analysis, Langchain, Streamlit, Banking Annual Reports.\", role='assistant', function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1714550326, model='gpt-4-32k', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=419, prompt_tokens=265, total_tokens=684), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])\n"
     ]
    }
   ],
   "source": [
    "import summarization\n",
    "content = summarization.content_generator(logit=res['text'][:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-9Jz9SOf2pEs5o09DeVEKoow3DYHsi', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"Title: Utilizing AI-Powered Large Language Models for Financial Analysis\\n\\nAbstract: \\nIn this paper, we present a novel application of AI-powered large language models for financial analysis. We describe the development and implementation of an application that leverages Langchain and Streamlit to analyze financial statements, with a specific focus on banking annual reports. This innovative approach provides a unique and efficient method for financial analysis, offering significant benefits over traditional methods.\\n\\nI. INTRODUCTION\\n\\nArtificial Intelligence (AI) has revolutionized numerous sectors, with finance being no exception. The use of AI-powered large language models, such as Langchain, for financial analysis offers a new perspective on how financial data can be interpreted and utilized. This paper provides an overview of the development of an application that leverages these models to analyze banking annual reports.\\n\\nII. METHODOLOGY\\n\\nOur approach involved the use of Langchain, a large language model, and Streamlit, a fast and easy way to build custom machine learning tools. The application was built to analyze a 300-page banking annual report, demonstrating the model's ability to handle large volumes of data.\\n\\nIII. IMPLEMENTATION\\n\\nThe application was developed without fine-tuning on the banking annual report. This demonstrates the inherent capability of large language models to adapt to specific tasks. The application allows for the upload of documents, such as meeting minutes or assignment outlines, and generates summaries or responses accordingly.\\n\\nIV. RESULTS AND DISCUSSION\\n\\nThe results of the application's analysis were highly accurate and insightful, providing a comprehensive financial analysis of the banking annual report. The use of AI-powered large language models for financial analysis offers significant benefits, including increased efficiency and the ability to handle large volumes of data.\\n\\nV. CONCLUSION\\n\\nThe application of AI-powered large language models for financial analysis presents a new and innovative approach to interpreting financial data. The development of this application demonstrates the potential of these models to revolutionize financial analysis, providing a unique and efficient method for analyzing financial statements.\\n\\nKeywords: AI-powered Large Language Models, Financial Analysis, Langchain, Streamlit, Banking Annual Reports.\", role='assistant', function_call=None, tool_calls=None), content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})], created=1714550326, model='gpt-4-32k', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=419, prompt_tokens=265, total_tokens=684), prompt_filter_results=[{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7011"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res['text'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "create_chunks() missing 1 required positional argument: 'timestamps'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     10\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is an AI agent doing financial analysis. I used it to build an LLM-Langchain app that could analyze financial statements. In fact, it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms doing it without fine-tuning on this 300-page banking annual report right now. I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm going to teach you how to build it in 5 minutes. I went down the rabbit hole to discover how to use your own documents to build AI-powered large language model apps with Langchain. And the results speak for themselves. But first of all, why should we even bother trying to use our own documents? We\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve already built our own large language model app with Streamlit. Why the need for docos? Well, it gives you the ultimate flexibility to leverage large language models for tasks that are specific to you. Provide meeting minutes and create a summary. You could upload an assignment outline and have a response generated. Upload an annual report and have a large language model do the financial analysis for you. In fact, that\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms what we\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre going to be doing we\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre going to be building our very own personal ai powered investment banker using streamlit and our own annual report to get started we first need to install some dependencies namely langchain openai streamlit tick token chroma db pi pdf and crypto dome while we\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre at it we\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre going to need to get an openai api key it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms optional to use the openai service you could substitute substitute it out for a Lama CPP or one of the Hugging Face hosted models. If you want a vid on that, let me know. Time to create our app. First up, in a new Python file called app.py, we\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre going to import some initial dependencies. OS is going to be used to set the API key. From langchain.llms, we\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll import the OpenAI service. This will be the core LLM we use. We\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll also bring in Streamlit while we\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre at it. Once that\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms done, we can set the API key and create the OpenAI service as an LLM. If we wanted to, we could tune the temperature depending on how creative or objective we wanted our responses. We also need somewhere to pass the prompt. We can use the Streamlit.TextInput element to do this. If our user hits enter, we need a way to send the prompt to the LLM. Let\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms throw in an if statement here and write out the text to the screen using streamlit.write. To start the app it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms relatively straightforward, just write streamlit run app.py or whatever your python file is named. Side note, all this code is going to be available in the description below so you can give it a crack yourself. This gives us a baseline app and uses open AI to generate responses. But there\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms one key issue here, we haven\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt gone and used our own documents yet. Let\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms fix that but before we do, a word from our sponsors. Me. If you\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md like to get up and running with Python for machine learning, head over to go.coursesfromnick.com forward slash Python, where you can take my end-to-end tech fundamentals course. Or if you want to dive straight into the deep end, you can check out my full stack machine learning course at this link here. And use YouTube 50 to get 50\u001b[39m\u001b[38;5;132;01m% o\u001b[39;00m\u001b[38;5;124mff right now. Back to the video. We need At the top of our script, we\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre going to bring in PyPDFLoader from langchain.documentloaders and Chroma from langchain.vectorstores. PyPDFLoader is used to load and pass a PDF into memory. Chroma is a vector store that is critical to using your own documents. What actually happens, as far as I know, is your document is tokenized and loaded into Chroma for querying later on. This allows you to perform similarity search using similarity metrics like Euclidean distance, similarity metrics like Euclidean distance. Also similar to how Pinecone works. We\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mll do this in a sec. All right, time to load up our document. Using the PyPDF loader class, we\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mre going to upload a banking annual report. I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mve got the files stored in the same place as my Python script, so I can just pass the name of the document direct to the class. If you wanted to use your own document, you could sub in the name of the file here. In this case, it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms got to be a PDF, be a PDF but there are other loaders available inside of Langchain. We can then load the document\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mcreate_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: create_chunks() missing 1 required positional argument: 'timestamps'"
     ]
    }
   ],
   "source": [
    "def create_chunks(text, timestamps):\n",
    "    chunks = []\n",
    "    for i in range(len(timestamps)):\n",
    "        start_time, end_time = timestamps[i]['timestamp']\n",
    "        chunk_text = text[start_time:end_time]\n",
    "        chunks.append({'timestamp': (start_time, end_time), 'text': chunk_text})\n",
    "    return chunks\n",
    "\n",
    "# Example usage\n",
    "text = \"This is an AI agent doing financial analysis. I used it to build an LLM-Langchain app that could analyze financial statements. In fact, it's doing it without fine-tuning on this 300-page banking annual report right now. I'm going to teach you how to build it in 5 minutes. I went down the rabbit hole to discover how to use your own documents to build AI-powered large language model apps with Langchain. And the results speak for themselves. But first of all, why should we even bother trying to use our own documents? We've already built our own large language model app with Streamlit. Why the need for docos? Well, it gives you the ultimate flexibility to leverage large language models for tasks that are specific to you. Provide meeting minutes and create a summary. You could upload an assignment outline and have a response generated. Upload an annual report and have a large language model do the financial analysis for you. In fact, that's what we're going to be doing we're going to be building our very own personal ai powered investment banker using streamlit and our own annual report to get started we first need to install some dependencies namely langchain openai streamlit tick token chroma db pi pdf and crypto dome while we're at it we're going to need to get an openai api key it's optional to use the openai service you could substitute substitute it out for a Lama CPP or one of the Hugging Face hosted models. If you want a vid on that, let me know. Time to create our app. First up, in a new Python file called app.py, we're going to import some initial dependencies. OS is going to be used to set the API key. From langchain.llms, we'll import the OpenAI service. This will be the core LLM we use. We'll also bring in Streamlit while we're at it. Once that's done, we can set the API key and create the OpenAI service as an LLM. If we wanted to, we could tune the temperature depending on how creative or objective we wanted our responses. We also need somewhere to pass the prompt. We can use the Streamlit.TextInput element to do this. If our user hits enter, we need a way to send the prompt to the LLM. Let's throw in an if statement here and write out the text to the screen using streamlit.write. To start the app it's relatively straightforward, just write streamlit run app.py or whatever your python file is named. Side note, all this code is going to be available in the description below so you can give it a crack yourself. This gives us a baseline app and uses open AI to generate responses. But there's one key issue here, we haven't gone and used our own documents yet. Let's fix that but before we do, a word from our sponsors. Me. If you'd like to get up and running with Python for machine learning, head over to go.coursesfromnick.com forward slash Python, where you can take my end-to-end tech fundamentals course. Or if you want to dive straight into the deep end, you can check out my full stack machine learning course at this link here. And use YouTube 50 to get 50% off right now. Back to the video. We need At the top of our script, we're going to bring in PyPDFLoader from langchain.documentloaders and Chroma from langchain.vectorstores. PyPDFLoader is used to load and pass a PDF into memory. Chroma is a vector store that is critical to using your own documents. What actually happens, as far as I know, is your document is tokenized and loaded into Chroma for querying later on. This allows you to perform similarity search using similarity metrics like Euclidean distance, similarity metrics like Euclidean distance. Also similar to how Pinecone works. We'll do this in a sec. All right, time to load up our document. Using the PyPDF loader class, we're going to upload a banking annual report. I've got the files stored in the same place as my Python script, so I can just pass the name of the document direct to the class. If you wanted to use your own document, you could sub in the name of the file here. In this case, it's got to be a PDF, be a PDF but there are other loaders available inside of Langchain. We can then load the document\"\n",
    "create_chunks(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Great() missing 1 required positional argument: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mGreat\u001b[39m(\u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m holder \u001b[38;5;241m=\u001b[39m \u001b[43mGreat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m holder(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mki\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Great() missing 1 required positional argument: 'input'"
     ]
    }
   ],
   "source": [
    "def Great(input):\n",
    "    print(input)\n",
    "holder = Great()\n",
    "holder(\"ki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't college students throw parties?\n",
      "\n",
      "Because they can't even clean up their own syllabus!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = \"https://openai-demetrius.openai.azure.com/\", \n",
    "  api_key= '99e288a8e7eb48fc91de6cc1f8e991d2',  \n",
    "  api_version=\"2024-02-15-preview\"\n",
    ")\n",
    "\n",
    "logits = \"tell me joke on college\"\n",
    "message_text = [{\"role\": \"user\", \"content\": logits}, {\"role\":\"system\",\"content\":\"You are an AI assistant that makes the Funny jokes based on content\"}]\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"gpt4-demetrius\", # model = \"deployment_name\"\n",
    "  messages = message_text,\n",
    "  temperature=0,\n",
    "  max_tokens=800,\n",
    "  top_p=0.95,\n",
    "  frequency_penalty=0,\n",
    "  presence_penalty=0,\n",
    "  stop=None\n",
    ")\n",
    "print(completion.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error loading [{'timestamp': (0.08, 2.36), 'text': ' This is an AI agent doing financial analysis.'}, {'timestamp': (2.36, 5.84), 'text': ' I used it to build an LLM-Langchain app that could analyze financial statements.'}, {'timestamp': (5.84, 9.92), 'text': \" In fact, it's doing it without fine-tuning on this 300-page banking annual report right\"}, {'timestamp': (9.92, 10.92), 'text': ' now.'}, {'timestamp': (10.92, 12.04), 'text': \" I'm going to teach you how to build it in 5 minutes.\"}, {'timestamp': (12.04, 15.84), 'text': ' I went down the rabbit hole to discover how to use your own documents to build AI-powered'}, {'timestamp': (15.84, 18.4), 'text': ' large language model apps with Langchain.'}, {'timestamp': (18.4, 25.8), 'text': \" And the results speak for themselves. But first of all, why should we even bother trying to use our own documents? We've already built our own large language model app with Streamlit.\"}, {'timestamp': (25.8, 26.8), 'text': ' Why the need for docos?'}, {'timestamp': (26.8, 30.74), 'text': ' Well, it gives you the ultimate flexibility to leverage large language models for tasks'}, {'timestamp': (30.74, 32.68), 'text': ' that are specific to you.'}, {'timestamp': (32.68, 34.26), 'text': ' Provide meeting minutes and create a summary.'}, {'timestamp': (34.26, 37.18), 'text': ' You could upload an assignment outline and have a response generated.'}, {'timestamp': (37.18, 50.24), 'text': \" Upload an annual report and have a large language model do the financial analysis for you. In fact, that's what we're going to be doing we're going to be building our very own personal ai powered investment banker using streamlit and our own annual report to get\"}, {'timestamp': (50.24, 54.88), 'text': ' started we first need to install some dependencies namely langchain openai streamlit tick token'}, {'timestamp': (54.88, 60.64), 'text': \" chroma db pi pdf and crypto dome while we're at it we're going to need to get an openai api key it's\"}, {'timestamp': (60.64, 65.22), 'text': ' optional to use the openai service you could substitute substitute it out for a Lama CPP or one'}, {'timestamp': (65.22, 68.46), 'text': ' of the Hugging Face hosted models. If you want a vid on that, let me know. Time to create our app.'}, {'timestamp': (68.5, 72.36), 'text': \" First up, in a new Python file called app.py, we're going to import some initial dependencies.\"}, {'timestamp': (72.6, 78.18), 'text': \" OS is going to be used to set the API key. From langchain.llms, we'll import the OpenAI service.\"}, {'timestamp': (86.12, 89.96), 'text': \" This will be the core LLM we use. We'll also bring in Streamlit while we're at it. Once that's done, we can set the API key and create the OpenAI service as an LLM. If we wanted to, we could tune the temperature depending on how creative or objective we\"}, {'timestamp': (89.96, 90.96), 'text': ' wanted our responses.'}, {'timestamp': (90.96, 93.1), 'text': ' We also need somewhere to pass the prompt.'}, {'timestamp': (93.1, 95.68), 'text': ' We can use the Streamlit.TextInput element to do this.'}, {'timestamp': (95.68, 106.94), 'text': \" If our user hits enter, we need a way to send the prompt to the LLM. Let's throw in an if statement here and write out the text to the screen using streamlit.write. To start the app it's relatively straightforward, just write streamlit run app.py or whatever\"}, {'timestamp': (106.94, 108.32), 'text': ' your python file is named.'}, {'timestamp': (108.32, 111.36), 'text': ' Side note, all this code is going to be available in the description below so you can give it'}, {'timestamp': (111.36, 112.36), 'text': ' a crack yourself.'}, {'timestamp': (112.36, 115.84), 'text': ' This gives us a baseline app and uses open AI to generate responses.'}, {'timestamp': (115.84, 119.76), 'text': \" But there's one key issue here, we haven't gone and used our own documents yet.\"}, {'timestamp': (119.76, 127.5), 'text': \" Let's fix that but before we do, a word from our sponsors. Me. If you'd like to get up and running with Python for machine learning, head over to go.coursesfromnick.com forward slash Python,\"}, {'timestamp': (127.5, 130.0), 'text': ' where you can take my end-to-end tech fundamentals course.'}, {'timestamp': (130.0, 132.18), 'text': ' Or if you want to dive straight into the deep end,'}, {'timestamp': (132.18, 134.38), 'text': ' you can check out my full stack machine learning course'}, {'timestamp': (134.38, 135.34), 'text': ' at this link here.'}, {'timestamp': (135.34, 137.8), 'text': ' And use YouTube 50 to get 50% off right now.'}, {'timestamp': (137.8, 146.8), 'text': \" Back to the video. We need At the top of our script, we're going to bring in PyPDFLoader from langchain.documentloaders and Chroma from langchain.vectorstores.\"}, {'timestamp': (147.22, 149.58), 'text': ' PyPDFLoader is used to load and pass a PDF into memory.'}, {'timestamp': (149.84, 153.14), 'text': ' Chroma is a vector store that is critical to using your own documents.'}, {'timestamp': (153.24, 156.68), 'text': ' What actually happens, as far as I know, is your document is tokenized and loaded into'}, {'timestamp': (156.68, 157.98), 'text': ' Chroma for querying later on.'}, {'timestamp': (158.04, 162.08), 'text': ' This allows you to perform similarity search using similarity metrics like Euclidean distance,'}, {'timestamp': (165.06, 168.44), 'text': \" similarity metrics like Euclidean distance. Also similar to how Pinecone works. We'll do this in a sec. All right, time to load up our document. Using the PyPDF loader class, we're going to\"}, {'timestamp': (168.44, 172.42), 'text': \" upload a banking annual report. I've got the files stored in the same place as my Python script,\"}, {'timestamp': (172.52, 176.72), 'text': ' so I can just pass the name of the document direct to the class. If you wanted to use your'}, {'timestamp': (176.72, 180.58), 'text': \" own document, you could sub in the name of the file here. In this case, it's got to be a PDF,\"}, {'timestamp': (184.0, 189.2), 'text': ' be a PDF but there are other loaders available inside of Langchain. We can then load the document into Chroma using the from documents method. To do this pass the pages from the PDF to the'}, {'timestamp': (189.2, 193.2), 'text': ' loader method. And then back to that similarity search. Using a Streamlit expander class we can'}, {'timestamp': (193.2, 197.44), 'text': ' search through the document in natural text and render the results to the screen. Say we ask about'}, {'timestamp': (197.44, 206.88), 'text': ' the performance of the bank. Chroma will return the relevant passages from the document loaded. These will eventually be passed through to the Langchain agent to generate a human-like response'}, {'timestamp': (206.88, 208.18), 'text': ' based on that context.'}, {'timestamp': (208.18, 209.56), 'text': \" But we don't need to do this manually.\"}, {'timestamp': (209.56, 211.9), 'text': ' We can use the VectorStore agent from Langchain.'}, {'timestamp': (211.9, 213.56), 'text': ' This will package it all up pretty nicely.'}, {'timestamp': (213.56, 215.12), 'text': ' Last set of imports, home stretch now.'}, {'timestamp': (215.12, 217.28), 'text': \" First up, we'll import create VectorStore agent,\"}, {'timestamp': (217.28, 225.48), 'text': ' VectorStore toolkit, and VectorStore info from Langchain.agents.agent toolkits. The vector store info class as far as I can tell just provides context about the'}, {'timestamp': (225.48, 229.64), 'text': \" store aka the pdf that we're going to pass through to our llm agent. You just pass your name,\"}, {'timestamp': (229.74, 234.82), 'text': ' description and chroma store to that specific class. Then the magic happens. We can pass the'}, {'timestamp': (234.82, 239.46), 'text': ' vector store info wrapper to vector store toolkit. This makes the pdf available as a tool to'}, {'timestamp': (239.46, 243.6), 'text': ' langchain similar to how we use the wikipedia toolkit in the langchain crash course video.'}, {'timestamp': (246.0, 246.16), 'text': ' And last but not least bring it it together with the VectorStore agent.'}, {'timestamp': (247.46, 247.88), 'text': ' This is the most critical part.'}, {'timestamp': (251.7, 253.32), 'text': ' So the agent creator class will pass through our original OpenAI LLM service and the VectorStore toolkit.'}, {'timestamp': (253.68, 256.36), 'text': ' This packages it all up nicely and will in effect'}, {'timestamp': (256.36, 259.26), 'text': ' give our large language model agent access to our PDF.'}, {'timestamp': (259.54, 262.64), 'text': ' Rather than just outputting responses from the LLM like we had previously,'}, {'timestamp': (269.36, 273.12), 'text': \" we can now use the agent and run the prompt through it and again write out the responses to the screen this allows us to ask things like what was the net profit of the company with the response that we're getting back from our gpt\"}, {'timestamp': (273.12, 280.16), 'text': ' investment banker being 4 706 million or 56 on the prior year which just so happens to match note 6'}, {'timestamp': (280.16, 286.22), 'text': ' on earnings per share in the notes to the financial statements. What initiatives did the bank take towards sustainability?'}, {'timestamp': (286.66, 291.28), 'text': ' Our model is calling out the Net Zero Asset Managers Initiative and the Net Zero Banking'}, {'timestamp': (291.28, 294.72), 'text': ' Alliance. This is pretty closely tied to the governance section in the financial statement'}, {'timestamp': (294.72, 298.0), 'text': ' as well. And last but not least, summarize the financial performance of the bank.'}, {'timestamp': (298.64, 303.62), 'text': ' The model calls out net profit after taxes increasing by 56% compared to the prior year'}, {'timestamp': (303.62, 305.7), 'text': ' and EPS increasing by 51%.'}, {'timestamp': (305.7, 309.4), 'text': ' This just so happens to match the letter from the chair of the board of the remuneration committee'}, {'timestamp': (309.4, 315.82), 'text': ' calling out those exact same NPAT and EPS numbers. The document uses roughly 300 pages in length so'}, {'timestamp': (315.82, 319.96), 'text': ' documents can take a little while to return a response. That being said in 45 lines of code'}, {'timestamp': (319.96, 323.74), 'text': \" we've got our very own personal investment banker. Not too shabby. If you want to check out the lane\"}, {'timestamp': (323.74, 325.82), 'text': ' crash course video I did go and click here.'}]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py:42\u001b[0m, in \u001b[0;36mTextLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     43\u001b[0m         text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Split the text into chunks\u001b[39;00m\n\u001b[0;32m      7\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m CharacterTextSplitter(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m texts \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(\u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:29\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m     28\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_community\\document_loaders\\text.py:58\u001b[0m, in \u001b[0;36mTextLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m     60\u001b[0m metadata \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)}\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m Document(page_content\u001b[38;5;241m=\u001b[39mtext, metadata\u001b[38;5;241m=\u001b[39mmetadata)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error loading [{'timestamp': (0.08, 2.36), 'text': ' This is an AI agent doing financial analysis.'}, {'timestamp': (2.36, 5.84), 'text': ' I used it to build an LLM-Langchain app that could analyze financial statements.'}, {'timestamp': (5.84, 9.92), 'text': \" In fact, it's doing it without fine-tuning on this 300-page banking annual report right\"}, {'timestamp': (9.92, 10.92), 'text': ' now.'}, {'timestamp': (10.92, 12.04), 'text': \" I'm going to teach you how to build it in 5 minutes.\"}, {'timestamp': (12.04, 15.84), 'text': ' I went down the rabbit hole to discover how to use your own documents to build AI-powered'}, {'timestamp': (15.84, 18.4), 'text': ' large language model apps with Langchain.'}, {'timestamp': (18.4, 25.8), 'text': \" And the results speak for themselves. But first of all, why should we even bother trying to use our own documents? We've already built our own large language model app with Streamlit.\"}, {'timestamp': (25.8, 26.8), 'text': ' Why the need for docos?'}, {'timestamp': (26.8, 30.74), 'text': ' Well, it gives you the ultimate flexibility to leverage large language models for tasks'}, {'timestamp': (30.74, 32.68), 'text': ' that are specific to you.'}, {'timestamp': (32.68, 34.26), 'text': ' Provide meeting minutes and create a summary.'}, {'timestamp': (34.26, 37.18), 'text': ' You could upload an assignment outline and have a response generated.'}, {'timestamp': (37.18, 50.24), 'text': \" Upload an annual report and have a large language model do the financial analysis for you. In fact, that's what we're going to be doing we're going to be building our very own personal ai powered investment banker using streamlit and our own annual report to get\"}, {'timestamp': (50.24, 54.88), 'text': ' started we first need to install some dependencies namely langchain openai streamlit tick token'}, {'timestamp': (54.88, 60.64), 'text': \" chroma db pi pdf and crypto dome while we're at it we're going to need to get an openai api key it's\"}, {'timestamp': (60.64, 65.22), 'text': ' optional to use the openai service you could substitute substitute it out for a Lama CPP or one'}, {'timestamp': (65.22, 68.46), 'text': ' of the Hugging Face hosted models. If you want a vid on that, let me know. Time to create our app.'}, {'timestamp': (68.5, 72.36), 'text': \" First up, in a new Python file called app.py, we're going to import some initial dependencies.\"}, {'timestamp': (72.6, 78.18), 'text': \" OS is going to be used to set the API key. From langchain.llms, we'll import the OpenAI service.\"}, {'timestamp': (86.12, 89.96), 'text': \" This will be the core LLM we use. We'll also bring in Streamlit while we're at it. Once that's done, we can set the API key and create the OpenAI service as an LLM. If we wanted to, we could tune the temperature depending on how creative or objective we\"}, {'timestamp': (89.96, 90.96), 'text': ' wanted our responses.'}, {'timestamp': (90.96, 93.1), 'text': ' We also need somewhere to pass the prompt.'}, {'timestamp': (93.1, 95.68), 'text': ' We can use the Streamlit.TextInput element to do this.'}, {'timestamp': (95.68, 106.94), 'text': \" If our user hits enter, we need a way to send the prompt to the LLM. Let's throw in an if statement here and write out the text to the screen using streamlit.write. To start the app it's relatively straightforward, just write streamlit run app.py or whatever\"}, {'timestamp': (106.94, 108.32), 'text': ' your python file is named.'}, {'timestamp': (108.32, 111.36), 'text': ' Side note, all this code is going to be available in the description below so you can give it'}, {'timestamp': (111.36, 112.36), 'text': ' a crack yourself.'}, {'timestamp': (112.36, 115.84), 'text': ' This gives us a baseline app and uses open AI to generate responses.'}, {'timestamp': (115.84, 119.76), 'text': \" But there's one key issue here, we haven't gone and used our own documents yet.\"}, {'timestamp': (119.76, 127.5), 'text': \" Let's fix that but before we do, a word from our sponsors. Me. If you'd like to get up and running with Python for machine learning, head over to go.coursesfromnick.com forward slash Python,\"}, {'timestamp': (127.5, 130.0), 'text': ' where you can take my end-to-end tech fundamentals course.'}, {'timestamp': (130.0, 132.18), 'text': ' Or if you want to dive straight into the deep end,'}, {'timestamp': (132.18, 134.38), 'text': ' you can check out my full stack machine learning course'}, {'timestamp': (134.38, 135.34), 'text': ' at this link here.'}, {'timestamp': (135.34, 137.8), 'text': ' And use YouTube 50 to get 50% off right now.'}, {'timestamp': (137.8, 146.8), 'text': \" Back to the video. We need At the top of our script, we're going to bring in PyPDFLoader from langchain.documentloaders and Chroma from langchain.vectorstores.\"}, {'timestamp': (147.22, 149.58), 'text': ' PyPDFLoader is used to load and pass a PDF into memory.'}, {'timestamp': (149.84, 153.14), 'text': ' Chroma is a vector store that is critical to using your own documents.'}, {'timestamp': (153.24, 156.68), 'text': ' What actually happens, as far as I know, is your document is tokenized and loaded into'}, {'timestamp': (156.68, 157.98), 'text': ' Chroma for querying later on.'}, {'timestamp': (158.04, 162.08), 'text': ' This allows you to perform similarity search using similarity metrics like Euclidean distance,'}, {'timestamp': (165.06, 168.44), 'text': \" similarity metrics like Euclidean distance. Also similar to how Pinecone works. We'll do this in a sec. All right, time to load up our document. Using the PyPDF loader class, we're going to\"}, {'timestamp': (168.44, 172.42), 'text': \" upload a banking annual report. I've got the files stored in the same place as my Python script,\"}, {'timestamp': (172.52, 176.72), 'text': ' so I can just pass the name of the document direct to the class. If you wanted to use your'}, {'timestamp': (176.72, 180.58), 'text': \" own document, you could sub in the name of the file here. In this case, it's got to be a PDF,\"}, {'timestamp': (184.0, 189.2), 'text': ' be a PDF but there are other loaders available inside of Langchain. We can then load the document into Chroma using the from documents method. To do this pass the pages from the PDF to the'}, {'timestamp': (189.2, 193.2), 'text': ' loader method. And then back to that similarity search. Using a Streamlit expander class we can'}, {'timestamp': (193.2, 197.44), 'text': ' search through the document in natural text and render the results to the screen. Say we ask about'}, {'timestamp': (197.44, 206.88), 'text': ' the performance of the bank. Chroma will return the relevant passages from the document loaded. These will eventually be passed through to the Langchain agent to generate a human-like response'}, {'timestamp': (206.88, 208.18), 'text': ' based on that context.'}, {'timestamp': (208.18, 209.56), 'text': \" But we don't need to do this manually.\"}, {'timestamp': (209.56, 211.9), 'text': ' We can use the VectorStore agent from Langchain.'}, {'timestamp': (211.9, 213.56), 'text': ' This will package it all up pretty nicely.'}, {'timestamp': (213.56, 215.12), 'text': ' Last set of imports, home stretch now.'}, {'timestamp': (215.12, 217.28), 'text': \" First up, we'll import create VectorStore agent,\"}, {'timestamp': (217.28, 225.48), 'text': ' VectorStore toolkit, and VectorStore info from Langchain.agents.agent toolkits. The vector store info class as far as I can tell just provides context about the'}, {'timestamp': (225.48, 229.64), 'text': \" store aka the pdf that we're going to pass through to our llm agent. You just pass your name,\"}, {'timestamp': (229.74, 234.82), 'text': ' description and chroma store to that specific class. Then the magic happens. We can pass the'}, {'timestamp': (234.82, 239.46), 'text': ' vector store info wrapper to vector store toolkit. This makes the pdf available as a tool to'}, {'timestamp': (239.46, 243.6), 'text': ' langchain similar to how we use the wikipedia toolkit in the langchain crash course video.'}, {'timestamp': (246.0, 246.16), 'text': ' And last but not least bring it it together with the VectorStore agent.'}, {'timestamp': (247.46, 247.88), 'text': ' This is the most critical part.'}, {'timestamp': (251.7, 253.32), 'text': ' So the agent creator class will pass through our original OpenAI LLM service and the VectorStore toolkit.'}, {'timestamp': (253.68, 256.36), 'text': ' This packages it all up nicely and will in effect'}, {'timestamp': (256.36, 259.26), 'text': ' give our large language model agent access to our PDF.'}, {'timestamp': (259.54, 262.64), 'text': ' Rather than just outputting responses from the LLM like we had previously,'}, {'timestamp': (269.36, 273.12), 'text': \" we can now use the agent and run the prompt through it and again write out the responses to the screen this allows us to ask things like what was the net profit of the company with the response that we're getting back from our gpt\"}, {'timestamp': (273.12, 280.16), 'text': ' investment banker being 4 706 million or 56 on the prior year which just so happens to match note 6'}, {'timestamp': (280.16, 286.22), 'text': ' on earnings per share in the notes to the financial statements. What initiatives did the bank take towards sustainability?'}, {'timestamp': (286.66, 291.28), 'text': ' Our model is calling out the Net Zero Asset Managers Initiative and the Net Zero Banking'}, {'timestamp': (291.28, 294.72), 'text': ' Alliance. This is pretty closely tied to the governance section in the financial statement'}, {'timestamp': (294.72, 298.0), 'text': ' as well. And last but not least, summarize the financial performance of the bank.'}, {'timestamp': (298.64, 303.62), 'text': ' The model calls out net profit after taxes increasing by 56% compared to the prior year'}, {'timestamp': (303.62, 305.7), 'text': ' and EPS increasing by 51%.'}, {'timestamp': (305.7, 309.4), 'text': ' This just so happens to match the letter from the chair of the board of the remuneration committee'}, {'timestamp': (309.4, 315.82), 'text': ' calling out those exact same NPAT and EPS numbers. The document uses roughly 300 pages in length so'}, {'timestamp': (315.82, 319.96), 'text': ' documents can take a little while to return a response. That being said in 45 lines of code'}, {'timestamp': (319.96, 323.74), 'text': \" we've got our very own personal investment banker. Not too shabby. If you want to check out the lane\"}, {'timestamp': (323.74, 325.82), 'text': ' crash course video I did go and click here.'}]"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "loader = TextLoader(res[\"chunks\"])\n",
    "\n",
    "# Split the text into chunks\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "texts = text_splitter.split_documents(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
